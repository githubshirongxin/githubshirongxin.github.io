---
layout: post
title: 五大主流分布式存储技术对比分析
---

ceph，GFS，HDFS，Swift，lustre

# https://mlog.club/article/22138


## 1,Ceph

应用最广泛的开源分布式存储平台

不存在单点故障

对象存储，文件存储，块存储

随着规模的扩大，性能不受影响

数据有强一致性，保证写完副本之后才返回。

去中心化，没有中心节点。

缺点： 
- 去中心化的方案，对技术团队要求较高
- Ceph扩容是，由于数据平均分布，会导致存储性能下降

 ![](/images/2020-07-13-13-38-49.png)
 ![](/images/2020-07-13-13-39-10.png)
 ![](/images/2020-07-13-13-42-29.png)
 ![](/images/2020-07-13-13-42-49.png)

 ## GFS

GFS是谷歌分布式文件存储系统，专门为大数据搜索设计。
闭源i系统
适用于大量顺序读取和顺序追加，例如大文件读写。特别是GB级别。
注重大文件读写的稳定性，而不是延时。

中心化架构，只有一个master是active状态。

1.GFS的主要架构

GFS 架构比较简单，一个 GFS 集群一般由一个 master 、多个 chunkserver 和多个 clients 组成。

在 GFS 中，所有文件被切分成若干个 chunk，每个 chunk 拥有唯一不变的标识（在 chunk 创建时，由 master 负责分配），所有 chunk 都实际存储在 chunkserver 的磁盘上。

为了容灾，每个 chunk 都会被复制到多个 chunkserve
 ![](/images/2020-07-13-13-44-32.png)

## HDFS

是GFS的简化版本。所以，也适合大文件。
 ![](/images/2020-07-13-13-46-19.png)

 分块更大，默认128M
 不支持并发，同一时刻只允许一个写入者
 过程一致性
 MasterHA ， 故障切换需要几分钟

 使用场景：
 - 大文件，大数据处理，数据GB，TB，PB级别
 - 适合流式文件，一次写入，多次读取
 - 文件一旦写入，不能修改。

不适合的场景：
- 低延时数据访问
- 小文件存储
- 并发写入、文件随机修改

## Swift

Openstack下有Nova子项目，Swift是该子项目的分布式对象存储服务

所有组件都课扩展，避免单点故障。

账户，容器，对象。三层。

 ![](/images/2020-07-13-13-50-07.png)

 原生的对象存储，不支持实时文件读写
 完全你对称架构，没有主节点。没有单点故障。
 openstack的子项目，适合云环境的部署

 Swift是最终一致性，高可用
 Ceph是强一致性


 ## Lustre

 开源。用用与超算领域

 支持数万个客户端，支持PB级别，单个文件支持320TB
 大文件读写分片优化
 缺少副本机制，有单点故障

 适合高性能计算领域，适合大文件连续读写。


 ## 几个技术的比较

  ![](/images/2020-07-13-13-54-40.png)